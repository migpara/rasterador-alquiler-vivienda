import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import (
    mean_absolute_error,
    mean_absolute_percentage_error,
    mean_squared_error
)
import joblib

# =========================
# CONFIGURACI√ìN
# =========================
ARCHIVO = "dataset_madrid_limpio_IA.csv"
MODELO_SALIDA = "modelo_tasadorv2.joblib"

pd.set_option('display.max_columns', None)

print("üè† ENTRENANDO IA DE ALQUILERES (RANDOM FOREST - VERSI√ìN FINAL)")

# =========================
# 1. CARGA DE DATOS
# =========================
try:
    df = pd.read_csv(ARCHIVO, sep=';')
    print(f"‚úÖ Datos cargados correctamente: {len(df)} pisos.")
except FileNotFoundError:
    print(f"‚ùå Error: No se encuentra '{ARCHIVO}'. Ejecuta primero el script de limpieza.")
    exit()

# =========================
# 2. TARGET (y) Y PREPARACI√ìN
# =========================
# Predecimos el LOG del precio para estabilidad
y = df['log_precio']

# Eliminamos columnas que NO deben ver la IA
columnas_a_borrar = [
    'precio',        # objetivo real
    'log_precio',    # objetivo transformado
    'precio_m2',     # TRAMPA (leakage)
    'titulo',        # texto
    'url'            # identificador
]
X_bruto = df.drop(columns=columnas_a_borrar)

# =========================
# 3. SEGURIDAD: FILTRAR BARRIOS SOLITARIOS
# =========================
# Antes del One-Hot, eliminamos barrios con 1 solo piso para evitar errores en stratify
conteo_barrios = df['barrio'].value_counts()
barrios_solitarios = conteo_barrios[conteo_barrios < 2].index

if len(barrios_solitarios) > 0:
    print(f"‚ö†Ô∏è Eliminando {len(barrios_solitarios)} barrios con 1 solo piso para poder estratificar.")
    # Filtramos el DataFrame original para mantener la coherencia
    df_seguro = df[~df['barrio'].isin(barrios_solitarios)].copy()
    
    # Recalculamos X e y con el df filtrado
    y = df_seguro['log_precio']
    X_bruto = df_seguro.drop(columns=columnas_a_borrar)
else:
    df_seguro = df

# =========================
# 4. ONE-HOT ENCODING
# =========================
print("üîÑ Transformando barrios a formato num√©rico...")
X = pd.get_dummies(X_bruto, columns=['barrio'], drop_first=True)

print(f"üß† Total de variables usadas por la IA: {X.shape[1]}")

# =========================
# 5. TRAIN / TEST SPLIT (ESTRATIFICADO)
# =========================
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=df_seguro['barrio']   # Usamos el df filtrado
)

print(f"üìö Train: {len(X_train)} pisos")
print(f"üìù Test:  {len(X_test)} pisos")

# =========================
# 6. ENTRENAMIENTO DEL MODELO
# =========================
print("üèãÔ∏è‚Äç‚ôÇÔ∏è Entrenando Random Forest...")

modelo = RandomForestRegressor(
    n_estimators=300,
    max_depth=15,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)

modelo.fit(X_train, y_train)

# =========================
# 7. EVALUACI√ìN (FIX DE ERROR)
# =========================
print("üß™ Evaluando modelo...")

# Predicciones
pred_log_test = modelo.predict(X_test)
pred_test = np.exp(pred_log_test)
y_real_test = np.exp(y_test)

pred_log_train = modelo.predict(X_train)
pred_train = np.exp(pred_log_train)
y_real_train = np.exp(y_train)

# M√©tricas
mae = mean_absolute_error(y_real_test, pred_test)
mape = mean_absolute_percentage_error(y_real_test, pred_test)

# ---Calculamos RMSE manualmente ---
mse = mean_squared_error(y_real_test, pred_test) # Calculamos error cuadr√°tico medio
rmse = np.sqrt(mse)                              # Y hacemos la ra√≠z cuadrada con Numpy
# ----------------------------------------------------

train_mape = mean_absolute_percentage_error(y_real_train, pred_train)

print("\n" + "=" * 45)
print("üìä RESULTADOS DEL MODELO")
print(f"üîπ MAE (Error medio):       {mae:.0f} ‚Ç¨")
print(f"üîπ RMSE (Desviaci√≥n):      {rmse:.0f} ‚Ç¨")
print(f"üîπ MAPE TEST:              {mape:.2%}")
print(f"üîπ MAPE TRAIN:             {train_mape:.2%}")
print("=" * 45)

# Interpretaci√≥n
if mape < 0.15:
    print("üåü EXCELENTE: IA a nivel tasador profesional")
elif mape < 0.20:
    print("‚úÖ MUY BUENA: v√°lida para detectar gangas")
else:
    print("‚ö†Ô∏è Mejorable: considera m√°s datos o features")

# =========================
# 8. IMPORTANCIA DE VARIABLES
# =========================
importancias = pd.Series(
    modelo.feature_importances_,
    index=X.columns
).sort_values(ascending=False)

print("\nüîç TOP 10 VARIABLES M√ÅS IMPORTANTES")
print(importancias.head(10))

# =========================
# 9. GUARDAR MODELO
# =========================
pack_modelo = {
    "modelo": modelo,
    "columnas": list(X.columns),
    "mae": mae,
    "mape": mape
}

joblib.dump(pack_modelo, MODELO_SALIDA)

print("\nüíæ Modelo guardado correctamente como:")
print(f"   üëâ {MODELO_SALIDA}")
print("üöÄ LISTO PARA DETECTAR GANGAS")